<success_metrics>

<feature_inputs>
WHAT YOU'RE BUILDING:
[Feature description]

WHY YOU'RE BUILDING IT:
- Problem it solves: [User pain]
- Expected benefit: [What improves]
- Strategic goal: [Company objective it supports]

WHO IT'S FOR:
[Target users]

CURRENT STATE:
[Relevant baseline metrics if you have them]
</feature_inputs>

<metrics_framework>

You define metrics that actually measure success. Your process:

STEP 1: Start with the "why"Don't start with "what can we measure"
Start with "what does success look like?"

Success definition:
- For users: [What gets better for them]
- For business: [What improves for company]

Example:
Feature: "Auto-save"
Success for users: Never lose work
Success for business: Fewer frustrated users, less support

STEP 2: Choose metric typeInput metrics (leading indicators):
Measure usage/engagement
- % of users who try feature
- Frequency of use
- Feature adoption rate

Pro: Can measure quickly
Con: Doesn't prove value

Output metrics (lagging indicators):
Measure actual outcomes
- Task completion time (decreased)
- Error rate (decreased)
- User retention (increased)
- Revenue (increased)

Pro: Proves actual value
Con: Takes time to measure

Best practice: Track both

STEP 3: Define primary metricYour ONE metric that matters mostGood primary metrics:
- Directly tied to value delivered
- Measurable within reasonable timeframe
- Sensitive (will move if feature works)
- Directional (clear if good or bad)

Bad primary metrics:
- Vanity (looks good but means nothing)
- Gamed easily (can fake success)
- Delayed too long (can't learn)

STEP 4: Define guardrail metricsWhat should NOT get worse:Common guardrails:
- Performance (load time)
- Reliability (error rates)
- Core flows (don't break key workflows)
- User satisfaction (NPS)

STEP 5: Set realistic targetsTarget framework:Baseline: [Current state]
Target: [What you're aiming for]
Timeline: [When you'll measure]

How to set targets:
- Look at past feature performance
- Industry benchmarks if available
- Directional improvement (any increase is good)
- Stretch but achievable

Avoid:
- Pulled from thin air
- Overly ambitious (sets up for failure)
- Too conservative (not worth building)

STEP 6: Plan measurementHow you'll track:
- What events to instrument
- Where to track them
- Dashboard to monitor
- Review cadence

When you'll know:
- Early signal (1 week)
- Clear signal (1 month)
- Long-term impact (3 months)

Now define metrics for the specific feature described, adapted to its context and goals.

</metrics_framework>

---

## Example Metrics Definition

(Adapt structure based on feature type)

### Feature Success Criteria

Feature: [Name]Goal: [What this achieves]Launch date: [When]

---

### Success Definition

For users:
[What gets better for them - specific and measurable]

For business:
[What improves for company - tied to goals]

We'll know it's working if:
[Concrete observable outcomes]

---

### Primary Metric

Metric: [Name and definition]

Why this metric:
[Why this is the right thing to measure]

Baseline: [Current state]Target: [Goal]Timeline: [When measured]

Calculation:
[How it's measured - formula if needed]

---

### Secondary Metrics

Metric 1: [Name]
- Current: [Baseline]
- Target: [Goal]
- Why tracking: [What this tells us]

Metric 2: [Name]
[Same structure]

---

### Guardrail Metrics

| Metric | Baseline | Must stay above/below | Why it matters |
|--------|----------|----------------------|----------------|
| [Metric 1] | [X] | [Threshold] | [Reason] |
| [Metric 2] | [Y] | [Threshold] | [Reason] |

---

### Measurement Plan

Events to track:
[Specific events and what they capture]

Dashboard: [Where monitored]

Review cadence:
- Week 1: [What we check]
- Month 1: [Assessment]
- Quarter 1: [Long-term view]

Decision criteria:
- Ship to 100% if: [Conditions]
- Iterate if: [Conditions]
- Rollback if: [Conditions]

---

### Learning Goals

Beyond metrics, we want to learn:
[Qualitative questions to answer]

How we'll learn:
[Surveys, interviews, analytics, etc.]

</success_metrics>